# ps239T-final-project
Final Project for PS239T - Visualizing climate change and impact 


1. Motivation for the project: 
Climate change is often misunderstood - not the fact that it's happening (though naysayers exist in droves), but the exact
rate of climate change and its impact continue to be heavily debated. While I'm not fluent in climate change modelling, 
through another course this semester, I've been familiarizing myself with various global circulation models (GCMs - used 
to understand climate change trends so far, and project changes in the future) as well as the policy frameworks around
mitigating the impact of climate change.

During the course of my research, I noticed that there was a visible lack of open data describing climate change, and 
whatever data was made available was extremely inaccessible to anyone who doesn't know programming languages - which 
incidentally, was me, about four months ago. The few visualizations that were available were piecemeal and did not
provide a comprehensive view of the differential impacts of climate change. 

So I started thinking about this project as being a starting point for putting together an interactive data visualization
platform that would incorporate data about average temperatures, precipitation, direct economic impacts, update policies
and tie-in global funding commitments for each country. Obviously, this is an ambitious project - one that I envision
working on for the next several months. 

For this project, I wanted to start by getting a sense of the data that was out there, relatively easily available and just
visualize some initial temperature changes and economic impact through a couple of different graphs and charts. 

2. Data sourcing and collection process:
Like I mentioned earlier, most of the work related to climate change is still largely in flux and fairly nascent, 
so the data generated by complex models are propreitary and not available for immediate consumption by "lay" audience. 
The one comprehensive open source data available was World Bank's climate change open data initiative. The World Bank
has put together a somewhat exhaustive dataset containing average temperatures for eight periods, spanning from 1920 and
projecting forward until 2100. This data can be accessed through an API. 

Given that this is a fairly recent initiative (and one of the first World Bank Open Data initiatives), the API is fairly
basic and simple to use. The downside however, is that it's fairly basic and simple - the REST-based API works (only)
through a base link, and does not accept specifications for parameters separately. Additionally, given that I was reading
up on policy briefs for different countries simultaneously while collecting data, a lot of my initial data collection 
process was staggered across multiple sessions. I didn't end up creating a function that could loop over some of the parameters
and make my code more efficient, but I was able to extract data for my initial list of 33 countries. 

I used all the concepts we learnt to extract API data in class. The step-by-step process is as follows:
a. I imported 4 modules in Python - requests, urllib, json and csv to help with various stages of data extraction process
b. I then used the base url provided by World Bank open data API page, and added parameters related to time period,
variable, global circulation model specification, future projection scenarios and country. All of these parameters needed
to be added to the base url as an extension and did not work as separate items in a parameters dictionary. 
c. Once I had this url ready, I assigned it to a simple variable name in order to make it easier to reference it
d. I then used the get method from requests module to output a url where I could view the data collected from the API, and
then converted it to text format using text method. The output was a list containing a dictionary with the data keys and
values. I repeated this process for all of the eight time periods for every country in my data set. 
e. I assigned the text data to a new variable that I could use later to compile all of the .json format data together and
create one big list of all of the data. I assigned this list to a new variable, and then called on that variable when
using methods within the csv module to extract the data into a csv file. 

The exact code along with comments are in an iPython notebook named "01_WB API data extraction" in PS239T folder in my
github repository.


3. Data cleaning, analysis and visualization:
Once I had my average temperature data, I decided to start by visualizing temperature trend from 1920s until 2100. I did
this for a few South Asian countries, and realized that the line plots I was generating weren't very helpful, because I had
temperature data that had been averaged over 10-20 years at a time, so the minor differences weren't very visible. 

So instead, I decided to dive into creating scatterplots that incorporated temperature variance data as well as 
economic impact data:
a. I read in the csv files in R, and then calculated variance in average temperatures for each country across the two scenarios
I was considering. The first scenario is a moderate effort scenario - where countries take some action to mitigate climate
change and therefore reap benefits of those actions by the end of the century. The second scenario is a weak effort one -
countries operate under "business-as-usual" assumption, and so temperatures rise faster towards the end of the 21st
century. Clearly, the variance in moderate scenarios would be lower than the variance in weak effort scenarios. 
b. Once I had the variances, I manually extracted data describing the drop in GDP for all the countries in my dataset
from 3 separate research papers (all PDFs). I created a list with all of the GDP numbers, and using the variances calculated
earlier, created a new dataframe in R. I also classified countries on the basis of continents. 
c. I then used ggplot2 to set up a bubbleplot for each continent. I used specific functionalities within ggplot2 package
to manipulate labels, legends and data positioning, to make the charts as easily readable as possible. 
d. I also wanted to start experimenting with some basic maps. So I decided to use the data I had to create simple
map visualizations. I primarily used rworldmaps package, and set up a spatial polygon dataframe to connect variance and GDP
data to specific countries on the map. The final map visualizations made it easier to view global data in one frame,
and I think, are also more readable than the scatterplots. 

The code along with extensive comments are in a R script file titled "02_R data cleaning_analysis_visualization code". The
graphs and the maps are saved as jpeg photos in the github repo. The individual country files are saved as "WB_data_countryISO3name"
in the github repo. 

4. Further work: 
Now that I've gotten a sense of the basic data, I next need to identify sources through which I can gain easy access to
micro-level data such as annual temperatures and precipitation patterns to feed into global circulation models and use
that as a base to project temperatures in the future. I also need to quantitatively model the impact of different policies
on GDP, poverty, ecological wealth/damage, and other multi-dimensional measures of outcomes. 

In terms of programming, I want to continue developing my skills with Python (especially for data extraction
using web-scraping and APIs), and become better at writing functions to perform repetitive tasks. I also want to 
start familiarizing myself with the nitty-gritties of working with spatial polygon dataframes, especially in terms of constructing
and manipulating them. Finally, I also figure that in order to create an interactive system, I'd need to start working with
HTML and try and translate what I've learnt so far to pick up my learning speed with HTML. 